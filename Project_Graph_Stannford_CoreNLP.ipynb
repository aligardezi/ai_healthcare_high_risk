{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDJuhKgcuuVN",
    "outputId": "3aac8fd4-dcf8-4bd5-a855-57069ff1a9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanfordcorenlp in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (3.9.1.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from stanfordcorenlp) (5.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from stanfordcorenlp) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->stanfordcorenlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->stanfordcorenlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->stanfordcorenlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->stanfordcorenlp) (2024.8.30)\n",
      "Requirement already satisfied: pycorenlp in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from pycorenlp) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->pycorenlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->pycorenlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->pycorenlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests->pycorenlp) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install stanfordcorenlp\n",
    "! pip install pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qeVsTUVF7X_E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# Start Stanford CoreNLP server\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-IYxjxJF7VMK"
   },
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    props = {'annotators': 'tokenize,ssplit,pos,lemma,ner',\n",
    "             'pipelineLanguage': 'en',\n",
    "             'outputFormat': 'json'}\n",
    "    result = nlp.annotate(text, properties=props)\n",
    "    # Process the output to extract entities\n",
    "    entities = []\n",
    "    try:\n",
    "        import json\n",
    "        output = json.loads(result)\n",
    "        for sentence in output['sentences']:\n",
    "            for entity in sentence['entitymentions']:\n",
    "                entities.append({'text': entity['text'], 'label': entity['ner']})\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error processing JSON output from Stanford CoreNLP: {e}\")\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "gfkPNUrOtLb4"
   },
   "outputs": [],
   "source": [
    "def extract_relationships(text):\n",
    "    props = {'annotators': 'tokenize,ssplit,pos,depparse,openie', \n",
    "             'pipelineLanguage': 'en'}\n",
    "    result = nlp.annotate(text, properties=props)\n",
    "    # Process the output to extract relationships\n",
    "    relationships = []\n",
    "    try:\n",
    "        import json\n",
    "        output = json.loads(result)\n",
    "        for sentence in output['sentences']:\n",
    "            for relation in sentence.get('openie', []):\n",
    "                relationships.append({\n",
    "                    'subject': relation['subject'],\n",
    "                    'relation': relation['relation'],\n",
    "                    'object': relation['object']\n",
    "                })\n",
    "    except:\n",
    "        print(\"Error processing JSON output from Stanford CoreNLP.\")\n",
    "\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kqGPlh5_3AG",
    "outputId": "c7425ca1-6dc4-40a6-abc1-f30e3ef4e010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from neo4j) (2024.1)\n",
      "Downloading neo4j-5.27.0-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.27.0\n"
     ]
    }
   ],
   "source": [
    "! pip install neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RE8X_gzr_3sO"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connect to Neo4j\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"hello1234\"))\n",
    "\n",
    "# Create nodes and relationships in Neo4j\n",
    "def create_graph(tx, entities, relationships):\n",
    "    # Add entities as nodes\n",
    "    #for entity in entities:\n",
    "    #    print(entity[\"text\"])\n",
    "    # tx.run(\"MERGE (e:Entity {name: $name})\", name=entity[\"text\"])\n",
    "    for rel in relationships:\n",
    "        tx.run(\"MERGE (e:Entity {name: $name})\", name=rel[\"subject\"])\n",
    "        tx.run(\"MERGE (e:Entity {name: $name})\", name=rel[\"object\"])\n",
    "\n",
    "    # Add relationships as edges\n",
    "    for rel in relationships:\n",
    "        tx.run(\"\"\"\n",
    "        MATCH (e1:Entity {name: $source}), (e2:Entity {name: $target})\n",
    "        MERGE (e1)-[:RELATIONSHIP {type: $relation}]->(e2)\n",
    "        \"\"\", source=rel[\"subject\"], target=rel[\"object\"], relation=rel[\"relation\"])\n",
    "\n",
    "# Populate the graph\n",
    "def build_neo4j_graph(entities, relationships):\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(create_graph, entities, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV7sQpou_fzu",
    "outputId": "1988d2c7-4e30-4b80-b0c4-d2b2c69a9ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: [{'text': 'John Doe', 'label': 'PERSON'}, {'text': '2', 'label': 'NUMBER'}, {'text': 'diabetes mellitus', 'label': 'CAUSE_OF_DEATH'}, {'text': 'hypertension', 'label': 'CAUSE_OF_DEATH'}, {'text': 'Smith', 'label': 'PERSON'}, {'text': \"St. John's Hospital\", 'label': 'ORGANIZATION'}]\n",
      "Relationships: [{'subject': 'John Doe', 'relation': 'was', 'object': 'diagnosed'}, {'subject': 'John Doe', 'relation': 'was diagnosed with', 'object': 'Type 2 diabetes mellitus'}, {'subject': 'John Doe', 'relation': 'was', 'object': \"diagnosed with Type 2 diabetes mellitus at St. John 's Hospital\"}, {'subject': 'St. John', 'relation': 'at', 'object': 'Hospital'}, {'subject': 'John Doe', 'relation': 'was diagnosed with', 'object': 'T2DM'}, {'subject': 'John Doe', 'relation': 'was diagnosed at', 'object': \"St. John 's Hospital\"}]\n"
     ]
    }
   ],
   "source": [
    "text = \"John Doe, was diagnosed with Type 2 diabetes mellitus (T2DM) and hypertension by Dr. Smith at St. John's Hospital\"\n",
    "entities = extract_entities(text)\n",
    "relationships = extract_relationships(text)\n",
    "\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Relationships:\", relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'Anatomy', 'relation': 'includes', 'object': 'structures'}, {'subject': 'term anatomy', 'relation': 'mean', 'object': 'gross anatomy'}, {'subject': 'term anatomy', 'relation': 'used by', 'object': 'itself'}, {'subject': 'term anatomy', 'relation': 'mean', 'object': 'anatomy'}, {'subject': 'Microscopic anatomy', 'relation': 'is study of', 'object': 'cells'}, {'subject': 'anatomy', 'relation': 'also called', 'object': 'histology'}, {'subject': 'anatomy', 'relation': 'called', 'object': 'histology'}, {'subject': 'study', 'relation': 'using', 'object': 'microscope'}, {'subject': 'anatomy', 'relation': 'is study of', 'object': 'cells'}, {'subject': 'Microscopic anatomy', 'relation': 'is', 'object': 'study'}, {'subject': 'Microscopic anatomy', 'relation': 'called', 'object': 'histology'}, {'subject': 'Microscopic anatomy', 'relation': 'also called', 'object': 'histology'}, {'subject': 'anatomy', 'relation': 'is', 'object': 'study'}, {'subject': 'Anatomy', 'relation': 'leads physician toward', 'object': \"understanding of patient 's disease\"}, {'subject': 'she', 'relation': 'carrying out', 'object': 'examination'}, {'subject': 'he', 'relation': 'carrying out', 'object': 'examination'}, {'subject': 'she', 'relation': 'using', 'object': 'most advanced imaging techniques'}, {'subject': 'she', 'relation': 'carrying out', 'object': 'physical examination'}, {'subject': 'he', 'relation': 'carrying out', 'object': 'physical examination'}, {'subject': 'she', 'relation': 'using', 'object': 'advanced imaging techniques'}, {'subject': 'patient', 'relation': 'of', 'object': 'disease'}, {'subject': 'Anatomy', 'relation': 'leads', 'object': 'physician'}, {'subject': 'she', 'relation': 'using', 'object': 'imaging techniques'}, {'subject': 'Anatomy', 'relation': 'leads physician toward', 'object': 'understanding'}, {'subject': 'Anatomy', 'relation': 'is', 'object': 'important'}, {'subject': 'Anatomy', 'relation': 'is', 'object': 'also important'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=r\"C:\\\\Users\\\\aliim\\projects\\\\files\\\\AI_Healthcare\\\\textbooks\\\\chunk\\\\Anatomy_Gray.jsonl\", split='train')\n",
    "index = 0\n",
    "for entry in dataset:\n",
    "    relationships = extract_relationships(entry[\"content\"])\n",
    "    print(relationships)\n",
    "    break;\n",
    "#   build_neo4j_graph(None, relationships)\n",
    "#    print(\"Done processing entry \" + str(index))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\aliim\\AppData\\Local\\Temp\\ipykernel_27996\\2673555890.py:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  file_path = \"C:\\\\Users\\\\aliim\\projects\\\\files\\\\AI_Healthcare\\\\textbooks\\\\chunk\\\\Anatomy_Gray\\\\data\" + str(index) + \".txt\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=r\"C:\\\\Users\\\\aliim\\projects\\\\files\\\\AI_Healthcare\\\\textbooks\\\\chunk\\\\Anatomy_Gray.jsonl\", split='train')\n",
    "index = 0\n",
    "for entry in dataset:\n",
    "    # Define the file path\n",
    "    file_path = \"C:\\\\Users\\\\aliim\\projects\\\\files\\\\AI_Healthcare\\\\textbooks\\\\chunk\\\\Anatomy_Gray\\\\data\" + str(index) + \".txt\"\n",
    "    # Open the file in write mode (\"w\")\n",
    "    with open(file_path, \"w\") as file:       \n",
    "        # Write the string to the file\n",
    "        file.write(entry[\"content\"])\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1ppYZfSG427"
   },
   "outputs": [],
   "source": [
    "def print_neo4j_graph():\n",
    "    \"\"\"\n",
    "    Prints the nodes and relationships in a Neo4j graph.\n",
    "\n",
    "    Args:\n",
    "        uri: The URI of the Neo4j database.\n",
    "        auth: Authentication credentials (username, password).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            # Get all nodes\n",
    "            result = session.run(\"MATCH (n) RETURN n\")\n",
    "            nodes = [record[\"n\"] for record in result]\n",
    "\n",
    "            # Get all relationships\n",
    "            result = session.run(\"MATCH ()-[r]->() RETURN r\")\n",
    "            relationships = [record[\"r\"] for record in result]\n",
    "\n",
    "            # Print nodes\n",
    "            print(\"Nodes:\")\n",
    "            for node in nodes:\n",
    "                print(f\"  - {node.id}: {node.labels} - {node.get('properties', {})}\") # Print Node ID, labels and properties\n",
    "\n",
    "            # Print relationships\n",
    "            print(\"\\nRelationships:\")\n",
    "            for relationship in relationships:\n",
    "                print(f\"  - {relationship.id}: {relationship.type} - ({relationship.start_node.id}) -> ({relationship.end_node.id}) - {relationship.get('properties', {})}\") # Print relationship ID, type, start and end node and properties\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MTAx9cqiAVXa"
   },
   "outputs": [],
   "source": [
    "def query_graph(entity_name):\n",
    "    with driver.session() as session:\n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Entity {name: $name})-[r]->(e2:Entity)\n",
    "        RETURN e1.name AS source, r.type AS relation, e2.name AS target\n",
    "        \"\"\"\n",
    "        results = session.run(query, name=entity_name)\n",
    "        return [{\"source\": record[\"source\"], \"relation\": record[\"relation\"], \"target\": record[\"target\"]} for record in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--mRaKqPAW4r"
   },
   "outputs": [],
   "source": [
    "def delete_graph():\n",
    "  with driver.session() as session:\n",
    "    query = \"\"\"\n",
    "    MATCH (n) DETACH DELETE n\n",
    "    \"\"\"\n",
    "    session.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2pXsdO_IC4D",
    "outputId": "7bade6d5-365e-4f5f-fd72-48a603972599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'Anatomy', 'relation': 'includes', 'target': 'structures'}, {'source': 'Anatomy', 'relation': 'leads physician toward', 'target': \"understanding of patient 's disease\"}, {'source': 'Anatomy', 'relation': 'leads', 'target': 'physician'}, {'source': 'Anatomy', 'relation': 'leads physician toward', 'target': 'understanding'}, {'source': 'Anatomy', 'relation': 'is', 'target': 'important'}, {'source': 'Anatomy', 'relation': 'is', 'target': 'also important'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'just memorization of lists of names'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'just memorization of lists of names'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'just memorization'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'just memorization'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'just memorization of lists'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'just memorization of lists'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'memorization of lists of names'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'memorization of lists of names'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'memorization'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'memorization'}, {'source': 'Anatomy', 'relation': 'is much more than', 'target': 'memorization of lists'}, {'source': 'Anatomy', 'relation': 'is more than', 'target': 'memorization of lists'}, {'source': 'Anatomy', 'relation': 'is', 'target': 'much more'}, {'source': 'Anatomy', 'relation': 'is', 'target': 'more'}, {'source': 'Anatomy', 'relation': 'can', 'target': 'can studied'}, {'source': 'Anatomy', 'relation': 'can', 'target': 'can studied following regional'}]\n"
     ]
    }
   ],
   "source": [
    "result = query_graph(\"Anatomy\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_wYb8KJAao5",
    "outputId": "326b9678-05ef-41bb-dac2-50d605a4f955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.56.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from openai) (4.6.2)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.0-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\aliim\\anaconda3\\envs\\ai_healthcare_project\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.56.0-py3-none-any.whl (389 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.0-cp312-none-win_amd64.whl (206 kB)\n",
      "Downloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 18.3 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: typing-extensions, jiter, distro, annotated-types, pydantic-core, pydantic, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.8.0 openai-1.56.0 pydantic-2.10.2 pydantic-core-2.27.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FFRUKIxHAc5H"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def query_openai(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Send a query to OpenAI's LLM and return the response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The query or instruction for the LLM.\n",
    "    - model (str): The model to use (e.g., \"gpt-4\", \"gpt-3.5-turbo\").\n",
    "    - temperature (float): Controls randomness (0.0 for deterministic, 1.0 for creative).\n",
    "    - max_tokens (int): Maximum number of tokens to generate in the response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The LLM response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gao2_Y4Aexd",
    "outputId": "39aa9030-4b27-4536-ba97-985600effb36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which of the following bones has a process that joins with the zygomatic bone to form the zygomatic arch?\n",
      "A - The maxillary bone\t\n",
      "B - The temporal bone\n",
      "C - The sphenoid bone\n",
      "D - The frontal bone\n",
      "\n",
      "Please use only the following information in the prompt to answer the question. Also please specify whether information from any other sources was used.\n",
      "zygomatic bone is bone\n",
      "zygomatic bone completes floor Beyond end\n",
      "zygomatic bone completes floor Beyond anterior end\n",
      "zygomatic bone completes floor\n",
      "zygomatic bone is visual centerpiece\n",
      "zygomatic bone bone with rounded lateral surface\n",
      "zygomatic bone is centerpiece\n",
      "zygomatic bone completes floor Beyond end of fissure\n",
      "zygomatic bone completes floor Beyond anterior end of fissure\n",
      "zygomatic bone completes floor of bony orbit\n",
      "zygomatic bone is quadrangular-shaped bone\n",
      "zygomatic arch extends from region\n",
      "zygomatic arch extends to bone\n",
      "zygomatic arch extends from region of joint\n",
      "zygomatic arch extends to zygomatic bone\n",
      "zygomatic arch extends from region of temporomandibular joint\n",
      "\n",
      "Response: Based on the provided information, the correct answer is:\n",
      "\n",
      "B - The temporal bone\n",
      "\n",
      "The information indicates that the zygomatic arch extends from the region of the temporomandibular joint, which is associated with the temporal bone. Therefore, the temporal bone has a process that joins with the zygomatic bone to form the zygomatic arch. No other external sources were used to answer this question.\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "question = \"\"\"\n",
    "Which of the following bones has a process that joins with the zygomatic bone to form the zygomatic arch?\n",
    "A - The maxillary bone\t\n",
    "B - The temporal bone\n",
    "C - The sphenoid bone\n",
    "D - The frontal bone\n",
    "\"\"\"\n",
    "#entities = extract_entities(question)\n",
    "\n",
    "#entities.append('Anatomy')\n",
    "#print(\"Entities:\", entities)\n",
    "#entityInfo = []\n",
    "#for entity in entities:\n",
    "#  result = query_graph(entity[\"text\"])\n",
    "#  for r in result:\n",
    "#    entityInfo.append(r[\"source\"] + \" \" + r[\"relation\"] + \" \" + r[\"target\"] + \"\\n\")\n",
    "\n",
    "entityInfo = []\n",
    "result = query_graph(\"zygomatic bone\")\n",
    "for r in result:\n",
    "  entityInfo.append(r[\"source\"] + \" \" + r[\"relation\"] + \" \" + r[\"target\"] + \"\\n\")\n",
    "  \n",
    "result = query_graph(\"zygomatic arch\")\n",
    "for r in result:\n",
    "  entityInfo.append(r[\"source\"] + \" \" + r[\"relation\"] + \" \" + r[\"target\"] + \"\\n\")\n",
    "\n",
    "prompt = question + \"\\nPlease use only the following information in the prompt to answer the question. Also please specify whether information from any other sources was used.\\n\" + \"\".join(entityInfo)\n",
    "print(prompt)\n",
    "\n",
    "response = query_openai(prompt)\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AI_Healthcare_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
